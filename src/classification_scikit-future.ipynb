{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Crypto Data\").getOrCreate()\n",
    "df_categorized = spark.read.csv(\"/home/jovyan/data/data_train_test.csv\", inferSchema=True, encoding='utf-8', header=True).cache()\n",
    "df_predict = spark.read.csv(\"/home/jovyan/data/data_predict.csv\", inferSchema=True, encoding='utf-8', header=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorized = df_categorized.filter(df_categorized['time'] >= dt(2017, 1, 1)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 5233|\n",
      "|    3|26065|\n",
      "|    4|11444|\n",
      "|    2|39205|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_categorized.filter(df_categorized['time'] >= dt(2017,1,1)).groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81947\n",
      "34808\n"
     ]
    }
   ],
   "source": [
    "print(df_categorized.filter(df_categorized['time'] >= dt(2017,1,1)).count())\n",
    "print(df_categorized.filter(df_categorized['time'] >= dt(2018,1,1)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_categorized.select('label').toPandas()\n",
    "features = df_categorized.drop('_c0', 'time', 'symbol', 'label', 'price+6h-high', 'close', 'btc-close', 'volumefrom', 'volumeto', 'btc-volumefrom', 'btc-volumeto').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price-0h', 'price-1h', 'price-2h', 'price-4h', 'price-5h', 'price-6h',\n",
       "       'price-8h', 'price-10h', 'price-12h', 'price-24h', 'price-48h',\n",
       "       'price-96h', 'price-192h', 'price-384h', 'price-768h', 'fromvol-0h',\n",
       "       'fromvol-1h', 'fromvol-2h', 'fromvol-4h', 'fromvol-6h', 'fromvol-8h',\n",
       "       'fromvol-10h', 'fromvol-16h', 'fromvol-24h', 'fromvol-48h',\n",
       "       'fromvol-96h', 'fromvol-192h', 'fromvol-384h', 'fromvol-768h',\n",
       "       'btc-volumefrom', 'btc-volumeto', 'btc-price-0h', 'btc-price-1h',\n",
       "       'btc-price-2h', 'btc-price-4h', 'btc-price-5h', 'btc-price-6h',\n",
       "       'btc-price-8h', 'btc-price-10h', 'btc-price-12h', 'btc-price-24h',\n",
       "       'btc-price-48h', 'btc-price-96h', 'btc-price-192h', 'btc-price-384h',\n",
       "       'btc-price-768h', 'btc-fromvol-0h', 'btc-fromvol-1h', 'btc-fromvol-2h',\n",
       "       'btc-fromvol-4h', 'btc-fromvol-6h', 'btc-fromvol-8h', 'btc-fromvol-10h',\n",
       "       'btc-fromvol-16h', 'btc-fromvol-24h', 'btc-fromvol-48h',\n",
       "       'btc-fromvol-96h', 'btc-fromvol-192h', 'btc-fromvol-384h',\n",
       "       'btc-fromvol-768h'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=4567, \n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "standscaler = StandardScaler()\n",
    "mlpmodel = MLPClassifier(hidden_layer_sizes=(400, 300, 200, 100, 10), solver='adam', activation='relu', learning_rate='adaptive', verbose=True, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('stdscale', standscaler),\n",
    "    ('model', mlpmodel)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'model__solver': ['adam'],\n",
    "    'model__activation': ['relu', 'logistic'],\n",
    "    'model__hidden_layer_sizes': [(400, 300, 200, 100, 10)],\n",
    "    'model__verbose': [True]\n",
    "}\n",
    "# (100, 40, 40, 40) (400, 300, 200, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe,cv=2,param_grid=param_grid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Iteration 1, loss = 1.14645616\n",
      "Iteration 2, loss = 1.11769938\n",
      "Iteration 3, loss = 1.10721532\n",
      "Iteration 4, loss = 1.09875653\n",
      "Iteration 5, loss = 1.08839185\n",
      "Iteration 6, loss = 1.07803149\n",
      "Iteration 7, loss = 1.06570646\n",
      "Iteration 8, loss = 1.05354429\n",
      "Iteration 9, loss = 1.03983006\n",
      "Iteration 10, loss = 1.02284519\n",
      "Iteration 11, loss = 1.00115783\n",
      "Iteration 12, loss = 0.98318235\n",
      "Iteration 13, loss = 0.95929713\n",
      "Iteration 14, loss = 0.93670174\n",
      "Iteration 15, loss = 0.91141967\n",
      "Iteration 16, loss = 0.89319255\n",
      "Iteration 17, loss = 0.85845948\n",
      "Iteration 18, loss = 0.83769615\n",
      "Iteration 19, loss = 0.80903893\n",
      "Iteration 20, loss = 0.78285564\n",
      "Iteration 21, loss = 0.75643991\n",
      "Iteration 22, loss = 0.72886208\n",
      "Iteration 23, loss = 0.70906640\n",
      "Iteration 24, loss = 0.68621992\n",
      "Iteration 25, loss = 0.66455169\n",
      "Iteration 26, loss = 0.64382755\n",
      "Iteration 27, loss = 0.62856896\n",
      "Iteration 28, loss = 0.60012344\n",
      "Iteration 29, loss = 0.58696752\n",
      "Iteration 30, loss = 0.57340235\n",
      "Iteration 31, loss = 0.55702451\n",
      "Iteration 32, loss = 0.53257736\n",
      "Iteration 33, loss = 0.52650604\n",
      "Iteration 34, loss = 0.51065491\n",
      "Iteration 35, loss = 0.49746875\n",
      "Iteration 36, loss = 0.48605915\n",
      "Iteration 37, loss = 0.46920687\n",
      "Iteration 38, loss = 0.45002108\n",
      "Iteration 39, loss = 0.44744639\n",
      "Iteration 40, loss = 0.43428037\n",
      "Iteration 41, loss = 0.43756739\n",
      "Iteration 42, loss = 0.41468739\n",
      "Iteration 43, loss = 0.41162486\n",
      "Iteration 44, loss = 0.39377676\n",
      "Iteration 45, loss = 0.38897691\n",
      "Iteration 46, loss = 0.38222660\n",
      "Iteration 47, loss = 0.36743660\n",
      "Iteration 48, loss = 0.37540359\n",
      "Iteration 49, loss = 0.37600305\n",
      "Iteration 50, loss = 0.36199575\n",
      "Iteration 51, loss = 0.35206997\n",
      "Iteration 52, loss = 0.34218034\n",
      "Iteration 53, loss = 0.32949802\n",
      "Iteration 54, loss = 0.32661192\n",
      "Iteration 55, loss = 0.32096374\n",
      "Iteration 56, loss = 0.31661465\n",
      "Iteration 57, loss = 0.31652990\n",
      "Iteration 58, loss = 0.33320534\n",
      "Iteration 59, loss = 0.31157068\n",
      "Iteration 60, loss = 0.31592906\n",
      "Iteration 61, loss = 0.30281649\n",
      "Iteration 62, loss = 0.29965901\n",
      "Iteration 63, loss = 0.28627689\n",
      "Iteration 64, loss = 0.28140625\n",
      "Iteration 65, loss = 0.27836541\n",
      "Iteration 66, loss = 0.28517156\n",
      "Iteration 67, loss = 0.27308356\n",
      "Iteration 68, loss = 0.27123800\n",
      "Iteration 69, loss = 0.27347613\n",
      "Iteration 70, loss = 0.25843936\n",
      "Iteration 71, loss = 0.26279330\n",
      "Iteration 72, loss = 0.26183766\n",
      "Iteration 73, loss = 0.27382366\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.16973539\n",
      "Iteration 2, loss = 1.12043138\n",
      "Iteration 3, loss = 1.10742366\n",
      "Iteration 4, loss = 1.09902707\n",
      "Iteration 5, loss = 1.08708735\n",
      "Iteration 6, loss = 1.07711239\n",
      "Iteration 7, loss = 1.06761371\n",
      "Iteration 8, loss = 1.05791145\n",
      "Iteration 9, loss = 1.04241598\n",
      "Iteration 10, loss = 1.02680698\n",
      "Iteration 11, loss = 1.00843966\n",
      "Iteration 12, loss = 0.99148786\n",
      "Iteration 13, loss = 0.97670262\n",
      "Iteration 14, loss = 0.95221829\n",
      "Iteration 15, loss = 0.93395914\n",
      "Iteration 16, loss = 0.90864728\n",
      "Iteration 17, loss = 0.88293742\n",
      "Iteration 18, loss = 0.86472814\n",
      "Iteration 19, loss = 0.83994310\n",
      "Iteration 20, loss = 0.81470318\n",
      "Iteration 21, loss = 0.79139598\n",
      "Iteration 22, loss = 0.77053355\n",
      "Iteration 23, loss = 0.75202347\n",
      "Iteration 24, loss = 0.73098049\n",
      "Iteration 25, loss = 0.70596402\n",
      "Iteration 26, loss = 0.68718211\n",
      "Iteration 27, loss = 0.66740079\n",
      "Iteration 28, loss = 0.65060011\n",
      "Iteration 29, loss = 0.63373449\n",
      "Iteration 30, loss = 0.61797184\n",
      "Iteration 31, loss = 0.60193254\n",
      "Iteration 32, loss = 0.57707084\n",
      "Iteration 33, loss = 0.57962689\n",
      "Iteration 34, loss = 0.56042616\n",
      "Iteration 35, loss = 0.55754193\n",
      "Iteration 36, loss = 0.54229220\n",
      "Iteration 37, loss = 0.51901430\n",
      "Iteration 38, loss = 0.49923545\n",
      "Iteration 39, loss = 0.50175136\n",
      "Iteration 40, loss = 0.48406002\n",
      "Iteration 41, loss = 0.47119200\n",
      "Iteration 42, loss = 0.46868091\n",
      "Iteration 43, loss = 0.44948992\n",
      "Iteration 44, loss = 0.45046140\n",
      "Iteration 45, loss = 0.44150758\n",
      "Iteration 46, loss = 0.43150313\n",
      "Iteration 47, loss = 0.43436726\n",
      "Iteration 48, loss = 0.40714970\n",
      "Iteration 49, loss = 0.40518693\n",
      "Iteration 50, loss = 0.40171904\n",
      "Iteration 51, loss = 0.38666519\n",
      "Iteration 52, loss = 0.38717329\n",
      "Iteration 53, loss = 0.37603598\n",
      "Iteration 54, loss = 0.36519795\n",
      "Iteration 55, loss = 0.37482280\n",
      "Iteration 56, loss = 0.36794837\n",
      "Iteration 57, loss = 0.36758961\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22794890\n",
      "Iteration 2, loss = 1.16913154\n",
      "Iteration 3, loss = 1.16788761\n",
      "Iteration 4, loss = 1.16786363\n",
      "Iteration 5, loss = 1.16788278\n",
      "Iteration 6, loss = 1.16770282\n",
      "Iteration 7, loss = 1.15076771\n",
      "Iteration 8, loss = 1.14147061\n",
      "Iteration 9, loss = 1.13761908\n",
      "Iteration 10, loss = 1.13380645\n",
      "Iteration 11, loss = 1.13271246\n",
      "Iteration 12, loss = 1.12883257\n",
      "Iteration 13, loss = 1.12600155\n",
      "Iteration 14, loss = 1.12417618\n",
      "Iteration 15, loss = 1.12275875\n",
      "Iteration 16, loss = 1.12121293\n",
      "Iteration 17, loss = 1.12051219\n",
      "Iteration 18, loss = 1.11972896\n",
      "Iteration 19, loss = 1.11869373\n",
      "Iteration 20, loss = 1.11798313\n",
      "Iteration 21, loss = 1.11727753\n",
      "Iteration 22, loss = 1.11642203\n",
      "Iteration 23, loss = 1.11542146\n",
      "Iteration 24, loss = 1.11401080\n",
      "Iteration 25, loss = 1.11274005\n",
      "Iteration 26, loss = 1.11172905\n",
      "Iteration 27, loss = 1.11218352\n",
      "Iteration 28, loss = 1.11042251\n",
      "Iteration 29, loss = 1.10926167\n",
      "Iteration 30, loss = 1.10916375\n",
      "Iteration 31, loss = 1.10786424\n",
      "Iteration 32, loss = 1.10685183\n",
      "Iteration 33, loss = 1.10647094\n",
      "Iteration 34, loss = 1.10608383\n",
      "Iteration 35, loss = 1.10404587\n",
      "Iteration 36, loss = 1.10414921\n",
      "Iteration 37, loss = 1.10307231\n",
      "Iteration 38, loss = 1.10264230\n",
      "Iteration 39, loss = 1.10221575\n",
      "Iteration 40, loss = 1.10102263\n",
      "Iteration 41, loss = 1.09916627\n",
      "Iteration 42, loss = 1.09830754\n",
      "Iteration 43, loss = 1.09841291\n",
      "Iteration 44, loss = 1.09646187\n",
      "Iteration 45, loss = 1.09548117\n",
      "Iteration 46, loss = 1.09505250\n",
      "Iteration 47, loss = 1.09424643\n",
      "Iteration 48, loss = 1.09249650\n",
      "Iteration 49, loss = 1.09077303\n",
      "Iteration 50, loss = 1.08966232\n",
      "Iteration 51, loss = 1.08917975\n",
      "Iteration 52, loss = 1.08884282\n",
      "Iteration 53, loss = 1.08724100\n",
      "Iteration 54, loss = 1.08602479\n",
      "Iteration 55, loss = 1.08481552\n",
      "Iteration 56, loss = 1.08379279\n",
      "Iteration 57, loss = 1.08165582\n",
      "Iteration 58, loss = 1.08070038\n",
      "Iteration 59, loss = 1.07829724\n",
      "Iteration 60, loss = 1.07883656\n",
      "Iteration 61, loss = 1.07718216\n",
      "Iteration 62, loss = 1.07575667\n",
      "Iteration 63, loss = 1.07407732\n",
      "Iteration 64, loss = 1.07339486\n",
      "Iteration 65, loss = 1.07139765\n",
      "Iteration 66, loss = 1.06857021\n",
      "Iteration 67, loss = 1.06805126\n",
      "Iteration 68, loss = 1.06595483\n",
      "Iteration 69, loss = 1.06498890\n",
      "Iteration 70, loss = 1.06407179\n",
      "Iteration 71, loss = 1.06128952\n",
      "Iteration 72, loss = 1.06097570\n",
      "Iteration 73, loss = 1.05873938\n",
      "Iteration 74, loss = 1.05745525\n",
      "Iteration 75, loss = 1.05527501\n",
      "Iteration 76, loss = 1.05543410\n",
      "Iteration 77, loss = 1.05282855\n",
      "Iteration 78, loss = 1.05109691\n",
      "Iteration 79, loss = 1.04955845\n",
      "Iteration 80, loss = 1.04697363\n",
      "Iteration 81, loss = 1.04650771\n",
      "Iteration 82, loss = 1.04326410\n",
      "Iteration 83, loss = 1.04220392\n",
      "Iteration 84, loss = 1.04020661\n",
      "Iteration 85, loss = 1.03910924\n",
      "Iteration 86, loss = 1.03827141\n",
      "Iteration 87, loss = 1.03534080\n",
      "Iteration 88, loss = 1.03507335\n",
      "Iteration 89, loss = 1.03200451\n",
      "Iteration 90, loss = 1.03124664\n",
      "Iteration 91, loss = 1.02701699\n",
      "Iteration 92, loss = 1.02502935\n",
      "Iteration 93, loss = 1.02640284\n",
      "Iteration 94, loss = 1.02361374\n",
      "Iteration 95, loss = 1.01799394\n",
      "Iteration 96, loss = 1.01860060\n",
      "Iteration 97, loss = 1.01612211\n",
      "Iteration 98, loss = 1.01220323\n",
      "Iteration 99, loss = 1.01143139\n",
      "Iteration 100, loss = 1.00849274\n",
      "Iteration 101, loss = 1.00538688\n",
      "Iteration 102, loss = 1.00508745\n",
      "Iteration 103, loss = 1.00343625\n",
      "Iteration 104, loss = 1.00022478\n",
      "Iteration 105, loss = 0.99639985\n",
      "Iteration 106, loss = 0.99654794\n",
      "Iteration 107, loss = 0.99176672\n",
      "Iteration 108, loss = 0.98972750\n",
      "Iteration 109, loss = 0.99004994\n",
      "Iteration 110, loss = 0.98582218\n",
      "Iteration 111, loss = 0.98754131\n",
      "Iteration 112, loss = 0.98270985\n",
      "Iteration 113, loss = 0.98115269\n",
      "Iteration 114, loss = 0.98004819\n",
      "Iteration 115, loss = 0.97668163\n",
      "Iteration 116, loss = 0.97291341\n",
      "Iteration 117, loss = 0.96981699\n",
      "Iteration 118, loss = 0.96768276\n",
      "Iteration 119, loss = 0.96866710\n",
      "Iteration 120, loss = 0.96386007\n",
      "Iteration 121, loss = 0.96200995\n",
      "Iteration 122, loss = 0.96132179\n",
      "Iteration 123, loss = 0.95828918\n",
      "Iteration 124, loss = 0.95567309\n",
      "Iteration 125, loss = 0.95243777\n",
      "Iteration 126, loss = 0.95217432\n",
      "Iteration 127, loss = 0.94646612\n",
      "Iteration 128, loss = 0.94610673\n",
      "Iteration 129, loss = 0.94396345\n",
      "Iteration 130, loss = 0.93815489\n",
      "Iteration 131, loss = 0.93702937\n",
      "Iteration 132, loss = 0.93573177\n",
      "Iteration 133, loss = 0.93570589\n",
      "Iteration 134, loss = 0.92991022\n",
      "Iteration 135, loss = 0.92689112\n",
      "Iteration 136, loss = 0.92631491\n",
      "Iteration 137, loss = 0.92435469\n",
      "Iteration 138, loss = 0.91826249\n",
      "Iteration 139, loss = 0.91577075\n",
      "Iteration 140, loss = 0.91766905\n",
      "Iteration 141, loss = 0.91342907\n",
      "Iteration 142, loss = 0.91329408\n",
      "Iteration 143, loss = 0.90797634\n",
      "Iteration 144, loss = 0.90689460\n",
      "Iteration 145, loss = 0.90347580\n",
      "Iteration 146, loss = 0.90071519\n",
      "Iteration 147, loss = 0.89617509\n",
      "Iteration 148, loss = 0.89285396\n",
      "Iteration 149, loss = 0.88950331\n",
      "Iteration 150, loss = 0.88915994\n",
      "Iteration 151, loss = 0.88778367\n",
      "Iteration 152, loss = 0.88608302\n",
      "Iteration 153, loss = 0.88179967\n",
      "Iteration 154, loss = 0.87747574\n",
      "Iteration 155, loss = 0.87759047\n",
      "Iteration 156, loss = 0.87211739\n",
      "Iteration 157, loss = 0.87118408\n",
      "Iteration 158, loss = 0.86983826\n",
      "Iteration 159, loss = 0.86229005\n",
      "Iteration 160, loss = 0.86179568\n",
      "Iteration 161, loss = 0.86242362\n",
      "Iteration 162, loss = 0.85446007\n",
      "Iteration 163, loss = 0.85318114\n",
      "Iteration 164, loss = 0.85201789\n",
      "Iteration 165, loss = 0.84783203\n",
      "Iteration 166, loss = 0.84419391\n",
      "Iteration 167, loss = 0.84441005\n",
      "Iteration 168, loss = 0.83614887\n",
      "Iteration 169, loss = 0.83784091\n",
      "Iteration 170, loss = 0.83515517\n",
      "Iteration 171, loss = 0.83064502\n",
      "Iteration 172, loss = 0.82861397\n",
      "Iteration 173, loss = 0.83244307\n",
      "Iteration 174, loss = 0.82441629\n",
      "Iteration 175, loss = 0.82105177\n",
      "Iteration 176, loss = 0.81567449\n",
      "Iteration 177, loss = 0.81326765\n",
      "Iteration 178, loss = 0.80962432\n",
      "Iteration 179, loss = 0.81205512\n",
      "Iteration 180, loss = 0.80536569\n",
      "Iteration 181, loss = 0.80419650\n",
      "Iteration 182, loss = 0.79747335\n",
      "Iteration 183, loss = 0.80209577\n",
      "Iteration 184, loss = 0.80014959\n",
      "Iteration 185, loss = 0.79240599\n",
      "Iteration 186, loss = 0.79030431\n",
      "Iteration 187, loss = 0.78528976\n",
      "Iteration 188, loss = 0.78612694\n",
      "Iteration 189, loss = 0.78122634\n",
      "Iteration 190, loss = 0.77865148\n",
      "Iteration 191, loss = 0.77578246\n",
      "Iteration 192, loss = 0.77222787\n",
      "Iteration 193, loss = 0.77307286\n",
      "Iteration 194, loss = 0.76723803\n",
      "Iteration 195, loss = 0.76264670\n",
      "Iteration 196, loss = 0.76232726\n",
      "Iteration 197, loss = 0.76344670\n",
      "Iteration 198, loss = 0.75669733\n",
      "Iteration 199, loss = 0.75380944\n",
      "Iteration 200, loss = 0.75707401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.23902877\n",
      "Iteration 2, loss = 1.16833600\n",
      "Iteration 3, loss = 1.16788182\n",
      "Iteration 4, loss = 1.16793294\n",
      "Iteration 5, loss = 1.16787499\n",
      "Iteration 6, loss = 1.16785774\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 16.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13826965\n",
      "Iteration 2, loss = 1.11011974\n",
      "Iteration 3, loss = 1.09655785\n",
      "Iteration 4, loss = 1.08502269\n",
      "Iteration 5, loss = 1.07046899\n",
      "Iteration 6, loss = 1.05407449\n",
      "Iteration 7, loss = 1.03750628\n",
      "Iteration 8, loss = 1.01660354\n",
      "Iteration 9, loss = 0.99599278\n",
      "Iteration 10, loss = 0.97215811\n",
      "Iteration 11, loss = 0.94799745\n",
      "Iteration 12, loss = 0.92339731\n",
      "Iteration 13, loss = 0.89921703\n",
      "Iteration 14, loss = 0.87674619\n",
      "Iteration 15, loss = 0.85381935\n",
      "Iteration 16, loss = 0.83195934\n",
      "Iteration 17, loss = 0.80933013\n",
      "Iteration 18, loss = 0.78923374\n",
      "Iteration 19, loss = 0.77038056\n",
      "Iteration 20, loss = 0.75183877\n",
      "Iteration 21, loss = 0.73081348\n",
      "Iteration 22, loss = 0.71314527\n",
      "Iteration 23, loss = 0.70020127\n",
      "Iteration 24, loss = 0.68424382\n",
      "Iteration 25, loss = 0.66596422\n",
      "Iteration 26, loss = 0.65393547\n",
      "Iteration 27, loss = 0.63940407\n",
      "Iteration 28, loss = 0.62521786\n",
      "Iteration 29, loss = 0.61264481\n",
      "Iteration 30, loss = 0.59885147\n",
      "Iteration 31, loss = 0.59079939\n",
      "Iteration 32, loss = 0.57945578\n",
      "Iteration 33, loss = 0.56524269\n",
      "Iteration 34, loss = 0.56032718\n",
      "Iteration 35, loss = 0.55231809\n",
      "Iteration 36, loss = 0.54033244\n",
      "Iteration 37, loss = 0.52854843\n",
      "Iteration 38, loss = 0.52705316\n",
      "Iteration 39, loss = 0.51747336\n",
      "Iteration 40, loss = 0.50357900\n",
      "Iteration 41, loss = 0.49948625\n",
      "Iteration 42, loss = 0.49355312\n",
      "Iteration 43, loss = 0.48237335\n",
      "Iteration 44, loss = 0.46909195\n",
      "Iteration 45, loss = 0.47373876\n",
      "Iteration 46, loss = 0.46716443\n",
      "Iteration 47, loss = 0.45623584\n",
      "Iteration 48, loss = 0.45061521\n",
      "Iteration 49, loss = 0.44246291\n",
      "Iteration 50, loss = 0.44742752\n",
      "Iteration 51, loss = 0.44406175\n",
      "Iteration 52, loss = 0.43024022\n",
      "Iteration 53, loss = 0.41994666\n",
      "Iteration 54, loss = 0.42031081\n",
      "Iteration 55, loss = 0.40959513\n",
      "Iteration 56, loss = 0.41738022\n",
      "Iteration 57, loss = 0.40388616\n",
      "Iteration 58, loss = 0.40513071\n",
      "Iteration 59, loss = 0.39568260\n",
      "Iteration 60, loss = 0.40623591\n",
      "Iteration 61, loss = 0.38071608\n",
      "Iteration 62, loss = 0.38448789\n",
      "Iteration 63, loss = 0.38789012\n",
      "Iteration 64, loss = 0.38030893\n",
      "Iteration 65, loss = 0.37658360\n",
      "Iteration 66, loss = 0.36419965\n",
      "Iteration 67, loss = 0.36701415\n",
      "Iteration 68, loss = 0.36088788\n",
      "Iteration 69, loss = 0.35703765\n",
      "Iteration 70, loss = 0.36459767\n",
      "Iteration 71, loss = 0.35419589\n",
      "Iteration 72, loss = 0.35071777\n",
      "Iteration 73, loss = 0.35395670\n",
      "Iteration 74, loss = 0.34526707\n",
      "Iteration 75, loss = 0.34327336\n",
      "Iteration 76, loss = 0.34810547\n",
      "Iteration 77, loss = 0.34045228\n",
      "Iteration 78, loss = 0.33735394\n",
      "Iteration 79, loss = 0.32811941\n",
      "Iteration 80, loss = 0.32861722\n",
      "Iteration 81, loss = 0.31886281\n",
      "Iteration 82, loss = 0.31695996\n",
      "Iteration 83, loss = 0.32838599\n",
      "Iteration 84, loss = 0.32690959\n",
      "Iteration 85, loss = 0.32112973\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "CPU times: user 54min 9s, sys: 17min 56s, total: 1h 12min 5s\n",
      "Wall time: 24min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = grid.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = \"Learning Curves MLP\"\n",
    "# plot_learning_curve(grid, title, X_train, y_train.values.ravel(), ylim=(0.0, 1.0), cv=2, n_jobs=1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53197071384990846"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, ..., 2, 3, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__activation': 'relu',\n",
       " 'model__hidden_layer_sizes': (400, 300, 200, 100, 10),\n",
       " 'model__solver': 'adam',\n",
       " 'model__verbose': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = df_predict.select('symbol').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = {\n",
    "    '1': 'decrease more than -4%',\n",
    "    '2': 'decrease between -4% and 0%',\n",
    "    '3': 'grow between 0% and 4%',\n",
    "    '4': 'grow more than 4%'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 2018-07-01 12:00:00, these are the predictions:\n",
      "\n",
      "EOS price was at 8.08$. \t In 6h might grow between 0% and 4%.\n",
      "LTC price was at 80.13$. \t In 6h might decrease between -4% and 0%.\n",
      "ETH price was at 454.29$. \t In 6h might grow between 0% and 4%.\n",
      "BCH price was at 741.46$. \t In 6h might grow between 0% and 4%.\n",
      "VEN price was at 2.63$. \t In 6h might grow between 0% and 4%.\n",
      "XLM price was at 0.2$. \t In 6h might grow between 0% and 4%.\n",
      "CVC price was at 0.18$. \t In 6h might decrease between -4% and 0%.\n",
      "XRP price was at 0.46$. \t In 6h might decrease between -4% and 0%.\n"
     ]
    }
   ],
   "source": [
    "print(\"At {}, these are the predictions:\".format(df_predict.select(df_predict['time']).collect()[0][0]))\n",
    "print(\"\")\n",
    "from IPython.core.display import display, HTML\n",
    "for row in symbols:\n",
    "    price = df_predict.filter(df_predict['symbol'] == row[0]).select('price-0h').collect()[0][0]\n",
    "    df_row = df_predict.filter(df_predict['symbol'] == row[0]).drop('_c0', 'time', 'symbol', 'label', 'price+6h-high', '_c0', 'time', 'close', 'btc-close', 'volumefrom', 'volumeto', 'btc-volumefrom', 'btc-volumeto' ).toPandas()\n",
    "    pred = predictor.predict(df_row)\n",
    "    print(\"{} price was at {}$. \\t In 6h might {}.\".format(row[0], round(price,2), pred_labels[str(pred[0])]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
